{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "COMP-6651 - Algorithm Design Techniques\n",
        "\n",
        "Project - Clustering Algorithms Analysis\n",
        "\n"
      ],
      "metadata": {
        "id": "SfI45T2-BGMv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import AffinityPropagation as SklearnAffinityPropagation\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n",
        "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
        "import os\n",
        "import kagglehub\n",
        "from sklearn.feature_selection import mutual_info_classif\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder, LabelEncoder\n",
        "from scipy.spatial.distance import cdist\n",
        "from sklearn.metrics import (calinski_harabasz_score, silhouette_score,\n",
        "                             davies_bouldin_score, adjusted_rand_score,\n",
        "                             normalized_mutual_info_score)\n"
      ],
      "metadata": {
        "id": "DkP_hTZO7hpf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing for AI Global Index Dataset\n",
        "def preprocess_ai_index(data):\n",
        "    # Separate numerical and categorical columns\n",
        "    numeric_cols = data.select_dtypes(include=[np.number]).columns\n",
        "    categorical_cols = data.select_dtypes(exclude=[np.number]).columns\n",
        "\n",
        "    scaler = MinMaxScaler()\n",
        "    num_scaled = scaler.fit_transform(data[numeric_cols]) if len(numeric_cols) > 0 else np.array([])\n",
        "\n",
        "    encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
        "    cat_encoded = encoder.fit_transform(data[categorical_cols]) if len(categorical_cols) > 0 else np.array([])\n",
        "\n",
        "    if num_scaled.size and cat_encoded.size:\n",
        "        return np.hstack((num_scaled, cat_encoded))\n",
        "    elif num_scaled.size:\n",
        "        return num_scaled\n",
        "    else:\n",
        "        return cat_encoded\n"
      ],
      "metadata": {
        "id": "rlloS6VIkfvA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path_iris = kagglehub.dataset_download(\"himanshunakrani/iris-dataset\")\n",
        "f_path_iris = os.path.join(path_iris, 'iris.csv')\n",
        "iris_df = pd.read_csv(f_path_iris)\n",
        "# Extract labels (species)\n",
        "iris_labels = iris_df['species']\n",
        "# Remove label from the features\n",
        "iris_features = iris_df.drop(columns=['species'])\n",
        "# Convert to numpy\n",
        "iris_data = iris_features.values\n",
        "\n",
        "path_ai_index = kagglehub.dataset_download(\"katerynameleshenko/ai-index\")\n",
        "f_path_ai_index = os.path.join(path_ai_index, 'AI_index_db.csv')\n",
        "ai_df = pd.read_csv(f_path_ai_index)\n",
        "ai_df = ai_df.dropna()\n",
        "# Extract the 'Cluster' column as the label\n",
        "ai_labels = ai_df['Cluster']\n",
        "# Remove the 'Cluster' column from the features\n",
        "ai_df_features = ai_df.drop(columns=['Cluster'])\n",
        "# Preprocess the remaining features\n",
        "ai_data = preprocess_ai_index(ai_df_features)\n",
        "print(ai_df.head())\n",
        "path_earthquakes= kagglehub.dataset_download(\"shreyasur965/recent-earthquakes\")\n",
        "f_path_earthquakes = os.path.join(path_earthquakes, 'earthquakes.csv')\n",
        "earthquake_df = pd.read_csv(f_path_earthquakes)\n",
        "earthquake_df = earthquake_df[['magnitude', 'felt', 'cdi','mmi','tsunami','sig','depth', 'latitude', 'longitude', 'alert']].dropna()\n",
        "# Extract the alert labels\n",
        "earthquake_data_alerts = earthquake_df['alert']\n",
        "alert_encoded = LabelEncoder().fit_transform(earthquake_data_alerts)\n",
        "# Remove the label from features\n",
        "earthquake_data_features = earthquake_df.drop(columns=['alert'])\n",
        "# Scale the numeric features\n",
        "earthquake_data = StandardScaler().fit_transform(earthquake_data_features)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2RwC-u4kjOs",
        "outputId": "3ba87637-7bb6-428e-d3be-381cea39c275"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/himanshunakrani/iris-dataset?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 0.98k/0.98k [00:00<00:00, 430kB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/katerynameleshenko/ai-index?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2.38k/2.38k [00:00<00:00, 4.43MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n",
            "                    Country  Talent  Infrastructure  Operating Environment  \\\n",
            "0  United States of America  100.00           94.02                  64.56   \n",
            "1                     China   16.51          100.00                  91.57   \n",
            "2            United Kingdom   39.65           71.43                  74.65   \n",
            "3                    Canada   31.28           77.05                  93.94   \n",
            "4                    Israel   35.76           67.58                  82.44   \n",
            "\n",
            "   Research  Development  Government Strategy  Commercial  Total score  \\\n",
            "0    100.00       100.00                77.39      100.00       100.00   \n",
            "1     71.42        79.97                94.87       44.02        62.92   \n",
            "2     36.50        25.03                82.82       18.91        40.93   \n",
            "3     30.67        25.78               100.00       14.88        40.19   \n",
            "4     32.63        27.96                43.91       27.33        39.89   \n",
            "\n",
            "         Region                Cluster  Income group   Political regime  \n",
            "0      Americas          Power players          High  Liberal democracy  \n",
            "1  Asia-Pacific          Power players  Upper middle   Closed autocracy  \n",
            "2        Europe  Traditional champions          High  Liberal democracy  \n",
            "3      Americas  Traditional champions          High  Liberal democracy  \n",
            "4   Middle East           Rising stars          High  Liberal democracy  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/shreyasur965/recent-earthquakes?dataset_version_number=3...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 214k/214k [00:00<00:00, 45.0MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class BASE_AffinityPropagation:\n",
        "    def __init__(self, damping=0.5, max_iter=200, convergence_iter=15, preference=None):\n",
        "        self.damping = damping\n",
        "        self.max_iter = max_iter\n",
        "        self.convergence_iter = convergence_iter\n",
        "        self.preference = preference\n",
        "        self.R = None\n",
        "        self.A = None\n",
        "        self.S = None\n",
        "        self.exemplars_ = None\n",
        "        self.labels_ = None\n",
        "\n",
        "    def _compute_similarity(self, X):\n",
        "        X = np.array(X)\n",
        "        n_samples = X.shape[0]\n",
        "        # Compute negative squared Euclidean distance\n",
        "        S = -np.square(np.linalg.norm(X[:, np.newaxis, :] - X, axis=2))\n",
        "        # Set preferences (diagonal)\n",
        "        if self.preference is None:\n",
        "            preference = np.median(S)\n",
        "        else:\n",
        "            preference = self.preference\n",
        "        np.fill_diagonal(S, preference)\n",
        "        return S\n",
        "\n",
        "    def _update_responsibilities(self, S, R, A):\n",
        "        AS = A + S\n",
        "        np.fill_diagonal(AS, -np.inf)\n",
        "        max_AS = np.max(AS, axis=1, keepdims=True)\n",
        "        new_R = S - max_AS\n",
        "        # Apply damping\n",
        "        new_R = self.damping * R + (1 - self.damping) * new_R\n",
        "        return new_R\n",
        "\n",
        "    def _update_availabilities(self, R, A):\n",
        "        n = R.shape[0]\n",
        "        new_A = np.zeros_like(A)\n",
        "        # Update a(k,k)\n",
        "        sum_max_R = np.sum(np.maximum(R, 0), axis=0)\n",
        "        diag_R = np.diag(R)\n",
        "        new_A_diag = sum_max_R - np.maximum(0, diag_R)\n",
        "        np.fill_diagonal(new_A, new_A_diag)\n",
        "\n",
        "        # Update a(n,k) for n != k\n",
        "        sum_max_R = np.sum(np.maximum(R, 0), axis=0)\n",
        "        diag_R = np.diag(R)\n",
        "        for k in range(n):\n",
        "            r_kk = diag_R[k]\n",
        "            sum_other = sum_max_R[k] - np.maximum(R[:, k], 0) - np.maximum(r_kk, 0)\n",
        "            values = r_kk + sum_other\n",
        "            new_A[:, k] = np.minimum(0, values)\n",
        "            new_A[k, k] = new_A_diag[k]  # Ensure diagonal is not overwritten\n",
        "\n",
        "        # Apply damping\n",
        "        new_A = self.damping * A + (1 - self.damping) * new_A\n",
        "        return new_A\n",
        "\n",
        "    def _get_exemplars(self, R, A):\n",
        "        diagonal_sum = np.diag(R) + np.diag(A)\n",
        "        exemplars = np.where(diagonal_sum >= 0)[0].tolist()\n",
        "        return exemplars\n",
        "\n",
        "    def fit(self, X):\n",
        "        X = np.array(X)\n",
        "        self.S = self._compute_similarity(X)\n",
        "        n_samples = X.shape[0]\n",
        "        self.R = np.zeros((n_samples, n_samples))\n",
        "        self.A = np.zeros((n_samples, n_samples))\n",
        "        exemplars_history = []\n",
        "        consecutive_no_changes = 0\n",
        "\n",
        "        for it in range(self.max_iter):\n",
        "            new_R = self._update_responsibilities(self.S, self.R, self.A)\n",
        "            new_A = self._update_availabilities(new_R, self.A)\n",
        "\n",
        "            current_exemplars = self._get_exemplars(new_R, new_A)\n",
        "            current_exemplars.sort()\n",
        "            self.R = new_R\n",
        "            self.A = new_A\n",
        "\n",
        "            if exemplars_history:\n",
        "                previous_exemplars = exemplars_history[-1]\n",
        "                previous_exemplars.sort()\n",
        "                if previous_exemplars == current_exemplars:\n",
        "                    consecutive_no_changes += 1\n",
        "                else:\n",
        "                    consecutive_no_changes = 0\n",
        "                    exemplars_history.append(current_exemplars)\n",
        "            else:\n",
        "                exemplars_history.append(current_exemplars)\n",
        "\n",
        "            if consecutive_no_changes >= self.convergence_iter:\n",
        "                break\n",
        "\n",
        "        self.exemplars_ = self._get_exemplars(self.R, self.A)\n",
        "        self.exemplars_.sort()\n",
        "        K = self.exemplars_\n",
        "        n_samples = X.shape[0]\n",
        "\n",
        "        # Assign labels\n",
        "        self.labels_ = -np.ones(n_samples, dtype=int)\n",
        "        exemplar_indices = {k: idx for idx, k in enumerate(K)}\n",
        "        for n in range(n_samples):\n",
        "            if n in exemplar_indices:\n",
        "                self.labels_[n] = exemplar_indices[n]\n",
        "            else:\n",
        "                max_sum = -np.inf\n",
        "                best_k_idx = -1\n",
        "                for idx, k in enumerate(K):\n",
        "                    current_sum = self.A[n, k] + self.R[n, k]\n",
        "                    if current_sum > max_sum:\n",
        "                        max_sum = current_sum\n",
        "                        best_k_idx = idx\n",
        "                self.labels_[n] = best_k_idx\n",
        "\n",
        "        return self\n"
      ],
      "metadata": {
        "id": "W8RGRvvHm74C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_diameter(X, labels):\n",
        "    \"\"\"\n",
        "    Computes the average 'diameter' across all clusters,\n",
        "    where 'diameter' of a cluster is the maximum distance\n",
        "    between any two points in that cluster.\n",
        "    \"\"\"\n",
        "    from scipy.spatial.distance import pdist, squareform\n",
        "\n",
        "    unique_labels = np.unique(labels)\n",
        "    diameters = []\n",
        "    for lbl in unique_labels:\n",
        "        cluster_points = X[labels == lbl]\n",
        "        if len(cluster_points) > 1:\n",
        "            # pairwise distances in the cluster\n",
        "            dist_matrix = squareform(pdist(cluster_points))\n",
        "            diameters.append(dist_matrix.max())\n",
        "        else:\n",
        "            # A single point has diameter 0\n",
        "            diameters.append(0.0)\n",
        "    return np.mean(diameters)\n",
        "\n",
        "\n",
        "def compute_split(X, labels):\n",
        "    \"\"\"\n",
        "    An example 'split' metric: ratio of the size of the largest cluster\n",
        "    to the size of the smallest cluster. If there's only one cluster, return 1.\n",
        "    \"\"\"\n",
        "    unique_labels, counts = np.unique(labels, return_counts=True)\n",
        "    if len(counts) < 2:\n",
        "        return 1.0\n",
        "    return counts.max() / counts.min()\n",
        "\n",
        "\n",
        "def evaluate_clustering(X, labels, true_labels=None):\n",
        "    \"\"\"\n",
        "    Compute a set of metrics for the given clustering labels.\n",
        "    Some metrics require ground truth (true_labels).\n",
        "    If no true_labels is provided, ARI and MI will be omitted.\n",
        "    Returns a dict of metric_name -> value.\n",
        "    \"\"\"\n",
        "    metrics_dict = {}\n",
        "\n",
        "    # Unsupervised metrics\n",
        "    metrics_dict[\"Silhouette\"] = silhouette_score(X, labels)\n",
        "    metrics_dict[\"Davies-Bouldin\"] = davies_bouldin_score(X, labels)\n",
        "    metrics_dict[\"Calinski-Harabasz\"] = calinski_harabasz_score(X, labels)\n",
        "    metrics_dict[\"Diameter\"] = compute_diameter(X, labels)\n",
        "    metrics_dict[\"Split\"] = compute_split(X, labels)\n",
        "\n",
        "    # If we have ground-truth labels, we can compute supervised metrics\n",
        "    if true_labels is not None:\n",
        "        metrics_dict[\"Adjusted Rand Index\"] = adjusted_rand_score(true_labels, labels)\n",
        "        # Using normalized_mutual_info_score as a measure of MI\n",
        "        metrics_dict[\"Mutual Information\"] = normalized_mutual_info_score(true_labels, labels)\n",
        "\n",
        "    return metrics_dict\n"
      ],
      "metadata": {
        "id": "GxH_MiMkM4Y0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def run_affinity_experiment(name, X, damping=0.5, max_iter=200, convergence_iter=15,\n",
        "                            preference=None, true_labels=None):\n",
        "    print(f\"----- {name} -----\")\n",
        "\n",
        "    # Custom Affinity Propagation Implementation\n",
        "    custom_ap = BASE_AffinityPropagation(\n",
        "        damping=damping,\n",
        "        max_iter=max_iter,\n",
        "        convergence_iter=convergence_iter,\n",
        "        preference=preference\n",
        "    )\n",
        "    custom_ap.fit(X)\n",
        "    custom_labels = custom_ap.labels_\n",
        "    n_custom_clusters = len(custom_ap.exemplars_)\n",
        "    print(f\"Custom AP: Number of clusters = {n_custom_clusters}\")\n",
        "\n",
        "    # Scikit-learn's Implementation\n",
        "    sklearn_ap = SklearnAffinityPropagation(\n",
        "        damping=damping,\n",
        "        max_iter=max_iter,\n",
        "        convergence_iter=convergence_iter,\n",
        "        preference=preference,\n",
        "        affinity='euclidean',\n",
        "        random_state=0\n",
        "    )\n",
        "    sklearn_ap.fit(X)\n",
        "    sklearn_labels = sklearn_ap.labels_\n",
        "    n_sklearn_clusters = len(sklearn_ap.cluster_centers_)\n",
        "    print(f\"Sklearn AP: Number of clusters = {n_sklearn_clusters}\")\n",
        "\n",
        "    # Evaluation Metrics\n",
        "    custom_metrics = evaluate_clustering(X, custom_labels, true_labels=true_labels)\n",
        "    sklearn_metrics = evaluate_clustering(X, sklearn_labels, true_labels=true_labels)\n",
        "\n",
        "    print(\"\\nCustom AP Metrics:\")\n",
        "    for k, v in custom_metrics.items():\n",
        "        print(f\"  {k}: {v:.4f}\")\n",
        "\n",
        "    print(\"\\nSklearn AP Metrics:\")\n",
        "    for k, v in sklearn_metrics.items():\n",
        "        print(f\"  {k}: {v:.4f}\")\n",
        "\n",
        "    print(\"-\" * 40)\n",
        "    return custom_labels, sklearn_labels\n",
        "\n"
      ],
      "metadata": {
        "id": "GxzPwmSaLcpo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Running Custom and Sklearn AP on Iris Dataset\")\n",
        "custom_labels_iris, sklearn_labels_iris = run_affinity_experiment(\n",
        "    \"Iris\",\n",
        "    iris_data,\n",
        "    damping=0.67,\n",
        "    preference=np.median(-np.square(np.linalg.norm(iris_data[:, np.newaxis] - iris_data, axis=2))) * 0.9,\n",
        "    true_labels=iris_labels\n",
        ")\n",
        "\n",
        "print(\"\\nRunning Custom and Sklearn AP on AI Global Index Dataset\")\n",
        "custom_labels_ai, sklearn_labels_ai = run_affinity_experiment(\n",
        "    \"AI Global Index\",\n",
        "    ai_data,\n",
        "    damping=0.7,\n",
        "    preference=np.median(-np.square(np.linalg.norm(ai_data[:, np.newaxis] - ai_data, axis=2)))*1.2,  # Median similarity\n",
        "    true_labels=ai_labels\n",
        ")\n",
        "\n",
        "print(\"\\nRunning Custom and Sklearn AP on Earthquake Dataset\")\n",
        "custom_labels_eq, sklearn_labels_eq = run_affinity_experiment(\n",
        "    \"Earthquake\",\n",
        "    earthquake_data,\n",
        "    damping=0.9,\n",
        "    preference=np.median(-np.square(np.linalg.norm(earthquake_data[:, np.newaxis] - earthquake_data, axis=2)))*0.9,  # Controls cluster count\n",
        "    true_labels=alert_encoded\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "va_dcAXonbO6",
        "outputId": "c296dbf5-e616-41b5-ed05-bd5ac7359f52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Custom and Sklearn AP on Iris Dataset\n",
            "----- Iris -----\n",
            "Custom AP: Number of clusters = 2\n",
            "Sklearn AP: Number of clusters = 7\n",
            "\n",
            "Custom AP Metrics:\n",
            "  Silhouette: 0.3212\n",
            "  Davies-Bouldin: 0.7742\n",
            "  Calinski-Harabasz: 106.4128\n",
            "  Diameter: 3.7179\n",
            "  Split: 3.0541\n",
            "  Adjusted Rand Index: 0.2485\n",
            "  Mutual Information: 0.3609\n",
            "\n",
            "Sklearn AP Metrics:\n",
            "  Silhouette: 0.4412\n",
            "  Davies-Bouldin: 0.9984\n",
            "  Calinski-Harabasz: 392.7419\n",
            "  Diameter: 1.5167\n",
            "  Split: 5.5556\n",
            "  Adjusted Rand Index: 0.5988\n",
            "  Mutual Information: 0.6900\n",
            "----------------------------------------\n",
            "\n",
            "Running Custom and Sklearn AP on AI Global Index Dataset\n",
            "----- AI Global Index -----\n",
            "Custom AP: Number of clusters = 5\n",
            "Sklearn AP: Number of clusters = 9\n",
            "\n",
            "Custom AP Metrics:\n",
            "  Silhouette: 0.1152\n",
            "  Davies-Bouldin: 1.9397\n",
            "  Calinski-Harabasz: 8.1790\n",
            "  Diameter: 2.6966\n",
            "  Split: 4.6000\n",
            "  Adjusted Rand Index: 0.0287\n",
            "  Mutual Information: 0.3040\n",
            "\n",
            "Sklearn AP Metrics:\n",
            "  Silhouette: 0.2012\n",
            "  Davies-Bouldin: 1.4626\n",
            "  Calinski-Harabasz: 8.4031\n",
            "  Diameter: 2.2274\n",
            "  Split: 6.3333\n",
            "  Adjusted Rand Index: 0.0280\n",
            "  Mutual Information: 0.3207\n",
            "----------------------------------------\n",
            "\n",
            "Running Custom and Sklearn AP on Earthquake Dataset\n",
            "----- Earthquake -----\n",
            "Custom AP: Number of clusters = 4\n",
            "Sklearn AP: Number of clusters = 32\n",
            "\n",
            "Custom AP Metrics:\n",
            "  Silhouette: 0.3286\n",
            "  Davies-Bouldin: 1.1114\n",
            "  Calinski-Harabasz: 124.6813\n",
            "  Diameter: 6.5546\n",
            "  Split: 641.0000\n",
            "  Adjusted Rand Index: 0.2691\n",
            "  Mutual Information: 0.1850\n",
            "\n",
            "Sklearn AP Metrics:\n",
            "  Silhouette: 0.3070\n",
            "  Davies-Bouldin: 1.1486\n",
            "  Calinski-Harabasz: 183.4627\n",
            "  Diameter: 2.9895\n",
            "  Split: 66.0000\n",
            "  Adjusted Rand Index: 0.0091\n",
            "  Mutual Information: 0.1076\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/cluster/_affinity_propagation.py:140: ConvergenceWarning: Affinity propagation did not converge, this model may return degenerate cluster centers and labels.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def select_top_k_features_for_alert(df, alert_col='alert', k=5):\n",
        "    \"\"\"\n",
        "    Filters features based on Mutual Information (MI) with the alert label.\n",
        "    Steps:\n",
        "      1) Drop rows with any missing values (to avoid errors).\n",
        "      2) Label-encode the alert column if it is categorical (e.g., text).\n",
        "      3) For each feature, compute MI against the alert.\n",
        "      4) Sort features by MI score, descending.\n",
        "      5) Return the top k feature names.\n",
        "\n",
        "    \"\"\"\n",
        "    data = df.dropna()\n",
        "\n",
        "    # Separate the target (alert) from the features\n",
        "    y = data[alert_col]\n",
        "    X_df = data.drop(columns=[alert_col])\n",
        "\n",
        "    # encode alert:\n",
        "    y_enc = LabelEncoder().fit_transform(y)\n",
        "\n",
        "    # mutual_info_classif calculates MI between each column and y_enc\n",
        "    mi_scores = mutual_info_classif(X_df.values, y_enc, discrete_features=False)\n",
        "\n",
        "    # Pair up (feature_name, mi_score)\n",
        "    feats_and_scores = list(zip(X_df.columns, mi_scores))\n",
        "\n",
        "    # Sort by MI descending\n",
        "    feats_and_scores.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # Pick top k\n",
        "    top_features = [f[0] for f in feats_and_scores[:k]]\n",
        "\n",
        "    return top_features"
      ],
      "metadata": {
        "id": "ZecNaWTBtxSH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "## 4.2 - Select the top 5 features for predicting alert"
      ],
      "metadata": {
        "id": "Jh9qfQ5JubF8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "top5_features = select_top_k_features_for_alert(\n",
        "    earthquake_df,  # the original DataFrame with 'alert' and other columns\n",
        "    alert_col='alert',\n",
        "    k=5\n",
        ")\n",
        "print(\"Top 5 features most predictive of 'alert':\", top5_features)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zpGdwLOquouw",
        "outputId": "504ac91c-d02d-4a86-def0-89e7fdd8ec30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 features most predictive of 'alert': ['sig', 'mmi', 'longitude', 'latitude', 'magnitude']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.3 - Re-run the clustering using only top-5 features"
      ],
      "metadata": {
        "id": "tCaQj6ZduuBx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Subset the DataFrame to those top-5 columns\n",
        "earthquake_df_top5 = earthquake_df[top5_features]\n",
        "\n",
        "# 2) Scale the new subset\n",
        "eq_top5_data = StandardScaler().fit_transform(earthquake_df_top5.values)\n",
        "\n",
        "print(\"Running Base/Sklearn Affinity on Earthquake Dataset with Top-5 Features\")\n",
        "_ = run_affinity_experiment(\n",
        "    \"Earthquake (Top-5 Features)\",\n",
        "    eq_top5_data,\n",
        "    damping=0.7,\n",
        "    preference=np.median(-np.square(np.linalg.norm(eq_top5_data[:, np.newaxis] - eq_top5_data, axis=2)))* 0.8,  # Controls cluster count\n",
        "    true_labels=alert_encoded\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2YGsbqYnuy1h",
        "outputId": "e023ed08-a341-4bbe-b35d-2b4fec2bd672"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Base/Sklearn Affinity on Earthquake Dataset with Top-5 Features\n",
            "----- Earthquake (Top-5 Features) -----\n",
            "Custom AP: Number of clusters = 153\n",
            "Sklearn AP: Number of clusters = 39\n",
            "\n",
            "Custom AP Metrics:\n",
            "  Silhouette: -0.1539\n",
            "  Davies-Bouldin: 1.4893\n",
            "  Calinski-Harabasz: 20.6830\n",
            "  Diameter: 0.4492\n",
            "  Split: 58.0000\n",
            "  Adjusted Rand Index: -0.0009\n",
            "  Mutual Information: 0.0797\n",
            "\n",
            "Sklearn AP Metrics:\n",
            "  Silhouette: 0.3841\n",
            "  Davies-Bouldin: 0.8613\n",
            "  Calinski-Harabasz: 298.9453\n",
            "  Diameter: 1.5026\n",
            "  Split: 37.5000\n",
            "  Adjusted Rand Index: 0.0134\n",
            "  Mutual Information: 0.1466\n",
            "----------------------------------------\n"
          ]
        }
      ]
    }
  ]
}