{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-learn-extra"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zPelrBhFZAbX",
        "outputId": "558b6ece-0201-4a81-c379-70dc8ccc115d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikit-learn-extra\n",
            "  Downloading scikit_learn_extra-0.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from scikit-learn-extra) (2.0.2)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn-extra) (1.14.1)\n",
            "Requirement already satisfied: scikit-learn>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn-extra) (1.6.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.23.0->scikit-learn-extra) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.23.0->scikit-learn-extra) (3.6.0)\n",
            "Downloading scikit_learn_extra-0.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: scikit-learn-extra\n",
            "Successfully installed scikit-learn-extra-0.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy==1.23.5\n",
        "!pip install --upgrade scikit-learn-extra"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 629
        },
        "id": "N9CP4lezd5d5",
        "outputId": "0221451f-db35-4bbc-ad8a-64722d347b14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy==1.23.5\n",
            "  Downloading numpy-1.23.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\n",
            "Downloading numpy-1.23.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "blosc2 3.2.1 requires numpy>=1.26, but you have numpy 1.23.5 which is incompatible.\n",
            "jax 0.5.2 requires numpy>=1.25, but you have numpy 1.23.5 which is incompatible.\n",
            "xarray 2025.1.2 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n",
            "albumentations 2.0.5 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\n",
            "chex 0.1.89 requires numpy>=1.24.1, but you have numpy 1.23.5 which is incompatible.\n",
            "scikit-image 0.25.2 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n",
            "albucore 0.0.23 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\n",
            "bigframes 1.42.0 requires numpy>=1.24.0, but you have numpy 1.23.5 which is incompatible.\n",
            "pymc 5.21.1 requires numpy>=1.25.0, but you have numpy 1.23.5 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 1.23.5 which is incompatible.\n",
            "treescope 0.1.9 requires numpy>=1.25.2, but you have numpy 1.23.5 which is incompatible.\n",
            "imbalanced-learn 0.13.0 requires numpy<3,>=1.24.3, but you have numpy 1.23.5 which is incompatible.\n",
            "jaxlib 0.5.1 requires numpy>=1.25, but you have numpy 1.23.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.23.5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "4db3b8bce94f48cbb50b0237addf21b1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/base_command.py\", line 179, in exc_logging_wrapper\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder, LabelEncoder\n",
        "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score, adjusted_rand_score, mutual_info_score, accuracy_score, normalized_mutual_info_score\n",
        "from scipy.spatial.distance import pdist, squareform, cdist\n",
        "import os\n",
        "import kagglehub\n",
        "# from sklearn.feature_selection import mutual_info_classif\n",
        "from sklearn_extra.cluster import KMedoids"
      ],
      "metadata": {
        "id": "gmeTD32e3nIx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path_iris = kagglehub.dataset_download(\"himanshunakrani/iris-dataset\")\n",
        "f_path_iris = os.path.join(path_iris, 'iris.csv')\n",
        "iris_df = pd.read_csv(f_path_iris)\n",
        "# Extract labels (species)\n",
        "iris_labels = iris_df['species']\n",
        "# Remove label from the features\n",
        "iris_features = iris_df.drop(columns=['species'])\n",
        "# Convert to numpy\n",
        "iris_data = iris_features.values\n",
        "\n",
        "\n",
        "# Preprocessing for AI Global Index Dataset\n",
        "def preprocess_ai_index(data):\n",
        "    # Separate numerical and categorical columns\n",
        "    numeric_cols = data.select_dtypes(include=[np.number]).columns\n",
        "    categorical_cols = data.select_dtypes(exclude=[np.number]).columns\n",
        "\n",
        "    scaler = MinMaxScaler()\n",
        "    num_scaled = scaler.fit_transform(data[numeric_cols]) if len(numeric_cols) > 0 else np.array([])\n",
        "\n",
        "    encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
        "    cat_encoded = encoder.fit_transform(data[categorical_cols]) if len(categorical_cols) > 0 else np.array([])\n",
        "\n",
        "    if num_scaled.size and cat_encoded.size:\n",
        "        return np.hstack((num_scaled, cat_encoded))\n",
        "    elif num_scaled.size:\n",
        "        return num_scaled\n",
        "    else:\n",
        "        return cat_encoded\n",
        "\n",
        "path_ai_index = kagglehub.dataset_download(\"katerynameleshenko/ai-index\")\n",
        "f_path_ai_index = os.path.join(path_ai_index, 'AI_index_db.csv')\n",
        "ai_df = pd.read_csv(f_path_ai_index)\n",
        "ai_df = ai_df.dropna()\n",
        "# Extract the 'Cluster' column as the label\n",
        "ai_labels = ai_df['Cluster']\n",
        "# Remove the 'Cluster' column from the features\n",
        "ai_df_features = ai_df.drop(columns=['Cluster'])\n",
        "# Preprocess the remaining features\n",
        "ai_data = preprocess_ai_index(ai_df_features)\n",
        "\n",
        "path_earthquakes= kagglehub.dataset_download(\"shreyasur965/recent-earthquakes\")\n",
        "f_path_earthquakes = os.path.join(path_earthquakes, 'earthquakes.csv')\n",
        "earthquake_df = pd.read_csv(f_path_earthquakes)\n",
        "earthquake_df = earthquake_df[['magnitude', 'felt', 'cdi','mmi','tsunami','sig','depth', 'latitude', 'longitude', 'alert']].dropna()\n",
        "# Extract the alert labels\n",
        "earthquake_data_alerts = earthquake_df['alert']\n",
        "alert_encoded = LabelEncoder().fit_transform(earthquake_data_alerts)\n",
        "# Remove the label from features\n",
        "earthquake_data_features = earthquake_df.drop(columns=['alert'])\n",
        "# Scale the numeric features\n",
        "earthquake_data = StandardScaler().fit_transform(earthquake_data_features)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VbtKw6pmWEdx",
        "outputId": "a095b3a7-43e5-44e5-af5d-dc3249c43d70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/himanshunakrani/iris-dataset?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 0.98k/0.98k [00:00<00:00, 1.92MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/katerynameleshenko/ai-index?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2.38k/2.38k [00:00<00:00, 1.93MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/shreyasur965/recent-earthquakes?dataset_version_number=3...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 214k/214k [00:00<00:00, 64.6MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BASE_KMEDOIDS:\n",
        "    def __init__(self, k, max_iters=100):\n",
        "        self.k = k\n",
        "        self.max_iters = max_iters\n",
        "\n",
        "    def fit(self, data):\n",
        "        self.data = data\n",
        "        self.is_dataframe = isinstance(data, pd.DataFrame)\n",
        "\n",
        "        if self.is_dataframe:\n",
        "            data_np = data.to_numpy()\n",
        "        else:\n",
        "            data_np = data\n",
        "\n",
        "        n_samples, n_features = data_np.shape\n",
        "        medoid_indices = np.random.choice(n_samples, self.k, replace=False)\n",
        "\n",
        "        # Handle medoid initialization correctly\n",
        "        self.medoids = data.iloc[medoid_indices].to_numpy() if self.is_dataframe else data[medoid_indices]\n",
        "        self.labels = np.zeros(n_samples, dtype=int)\n",
        "\n",
        "        for _ in range(self.max_iters):\n",
        "            prev_labels = self.labels.copy()\n",
        "\n",
        "            # Compute Manhattan distances\n",
        "            distances = np.sum(np.abs(data_np[:, np.newaxis] - self.medoids), axis=2)\n",
        "            self.labels = np.argmin(distances, axis=1)\n",
        "\n",
        "            # Update medoids\n",
        "            for i in range(self.k):\n",
        "                cluster_indices = np.where(self.labels == i)[0]\n",
        "                if len(cluster_indices) == 0:\n",
        "                    continue\n",
        "\n",
        "                cluster_points = data_np[cluster_indices]\n",
        "                min_cost = float('inf')\n",
        "                best_medoid = None\n",
        "\n",
        "                for idx in cluster_indices:\n",
        "                    cost = np.sum(np.sum(np.abs(data_np[idx] - cluster_points), axis=1))\n",
        "                    if cost < min_cost:\n",
        "                        min_cost = cost\n",
        "                        best_medoid = data_np[idx]\n",
        "\n",
        "                self.medoids[i] = best_medoid\n",
        "\n",
        "            # Convergence check\n",
        "            if np.array_equal(self.labels, prev_labels):\n",
        "                break\n",
        "\n",
        "        return self.labels, self.medoids\n",
        "\n",
        "    def _to_numpy(self, d):\n",
        "      if self.is_dataframe:\n",
        "          # Only call .values if d is a DataFrame or Series\n",
        "          return d.values if hasattr(d, 'values') else np.array(d)\n",
        "      return d\n",
        "\n",
        "    def _get_row(self, idx):\n",
        "        return self.data.iloc[idx].values if self.is_dataframe else self.data[idx]\n",
        "\n",
        "    def _get_rows(self, indices):\n",
        "        return self._to_numpy(self.data.iloc[indices]) if self.is_dataframe else self.data[indices]\n"
      ],
      "metadata": {
        "id": "EtGMi8IIWcx7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sklearn_kmedoids(features, n_clusters):\n",
        "    model = KMedoids(n_clusters=n_clusters, random_state=42)\n",
        "    return model.fit_predict(features)"
      ],
      "metadata": {
        "id": "mNha23KxW9p1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_diameter(X, labels):\n",
        "    \"\"\"\n",
        "    Computes the average 'diameter' across all clusters,\n",
        "    where 'diameter' of a cluster is the maximum distance\n",
        "    between any two points in that cluster.\n",
        "    \"\"\"\n",
        "    unique_labels = np.unique(labels)\n",
        "    diameters = []\n",
        "    for lbl in unique_labels:\n",
        "        cluster_points = X[labels == lbl]\n",
        "        if len(cluster_points) > 1:\n",
        "            # pairwise distances in the cluster\n",
        "            dist_matrix = squareform(pdist(cluster_points))\n",
        "            diameters.append(dist_matrix.max())\n",
        "        else:\n",
        "            # A single point has diameter 0\n",
        "            diameters.append(0.0)\n",
        "    return np.mean(diameters)\n",
        "\n",
        "\n",
        "def compute_split(X, labels):\n",
        "    \"\"\"\n",
        "    An example 'split' metric: ratio of the size of the largest cluster\n",
        "    to the size of the smallest cluster. If there's only one cluster, return 1.\n",
        "    \"\"\"\n",
        "    unique_labels, counts = np.unique(labels, return_counts=True)\n",
        "    if len(counts) < 2:\n",
        "        return 1.0\n",
        "    return counts.max() / counts.min()\n",
        "\n",
        "\n",
        "def evaluate_clustering(X, labels, true_labels=None):\n",
        "    \"\"\"\n",
        "    Compute a set of metrics for the given clustering labels.\n",
        "    Some metrics require ground truth (true_labels).\n",
        "    If no true_labels is provided, ARI and MI will be omitted.\n",
        "    Returns a dict of metric_name -> value.\n",
        "    \"\"\"\n",
        "    metrics_dict = {}\n",
        "\n",
        "    # Unsupervised metrics\n",
        "    metrics_dict[\"Silhouette\"] = silhouette_score(X, labels)\n",
        "    metrics_dict[\"Davies-Bouldin\"] = davies_bouldin_score(X, labels)\n",
        "    metrics_dict[\"Calinski-Harabasz\"] = calinski_harabasz_score(X, labels)\n",
        "    metrics_dict[\"Diameter\"] = compute_diameter(X, labels)\n",
        "    metrics_dict[\"Split\"] = compute_split(X, labels)\n",
        "\n",
        "    # If we have ground-truth labels, we can compute supervised metrics\n",
        "    if true_labels is not None:\n",
        "        metrics_dict[\"Adjusted Rand Index\"] = adjusted_rand_score(true_labels, labels)\n",
        "        # Using normalized_mutual_info_score as a measure of MI\n",
        "        metrics_dict[\"Mutual Information\"] = normalized_mutual_info_score(true_labels, labels)\n",
        "    return metrics_dict"
      ],
      "metadata": {
        "id": "emVj7N2kWvWL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def kmedoids_experimentation(dataset_name, features, target, k):\n",
        "    print(f\"\\nProcessing {dataset_name} Dataset\")\n",
        "\n",
        "    base_labels,_ = BASE_KMEDOIDS(k).fit(features)\n",
        "\n",
        "    # Evaluate your BASE_KMEDOIDS using the evaluate_clustering function\n",
        "    # base_metrics = evaluate_clustering(features, base_labels, true_labels=target.values)\n",
        "    base_metrics = evaluate_clustering(features, base_labels, true_labels=target)\n",
        "    print(\"BASE K-Means Metrics:\")\n",
        "    for t, v in base_metrics.items():\n",
        "        print(f\"  {t}: {v:.4f}\")\n",
        "\n",
        "    # Scikit-learn DBSCAN\n",
        "    sklearn_labels = sklearn_kmedoids(features,k)\n",
        "\n",
        "    # Evaluate scikit-learn's KMedoids using the evaluate_clustering function\n",
        "    # sklearn_metrics = evaluate_clustering(features, sklearn_labels, true_labels=target.values)\n",
        "    sklearn_metrics = evaluate_clustering(features, sklearn_labels, true_labels=target)\n",
        "\n",
        "    print(\"Scikit-learn K-Medoids Metrics:\")\n",
        "    for t, v in sklearn_metrics.items():\n",
        "        print(f\"  {t}: {v:.4f}\")\n",
        "    return base_labels, sklearn_labels\n"
      ],
      "metadata": {
        "id": "s-RgTMHZWyhR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_labels_iris, sklearn_labels_iris = kmedoids_experimentation(\"Iris\", iris_data, iris_labels, 3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jm8b2x5QW1It",
        "outputId": "4a05ba43-87cd-47dc-f194-148577cc873c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing Iris Dataset\n",
            "BASE K-Means Metrics:\n",
            "  Silhouette: 0.5435\n",
            "  Davies-Bouldin: 0.6760\n",
            "  Calinski-Harabasz: 552.2292\n",
            "  Diameter: 2.6089\n",
            "  Split: 1.5000\n",
            "  Adjusted Rand Index: 0.7028\n",
            "  Mutual Information: 0.7277\n",
            "Scikit-learn K-Medoids Metrics:\n",
            "  Silhouette: 0.5200\n",
            "  Davies-Bouldin: 0.6690\n",
            "  Calinski-Harabasz: 520.4530\n",
            "  Diameter: 2.5166\n",
            "  Split: 1.6316\n",
            "  Adjusted Rand Index: 0.7583\n",
            "  Mutual Information: 0.7857\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ai_global_base_labels, ai_global_sklearn_labels = kmedoids_experimentation('AI Global Index', ai_data, ai_labels, 5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dUF5r9CBW1Ar",
        "outputId": "7c67a5a4-23a0-4a16-ae8c-27386dd36d5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing AI Global Index Dataset\n",
            "BASE K-Means Metrics:\n",
            "  Silhouette: 0.1470\n",
            "  Davies-Bouldin: 1.9280\n",
            "  Calinski-Harabasz: 8.3775\n",
            "  Diameter: 2.7597\n",
            "  Split: 7.0000\n",
            "  Adjusted Rand Index: 0.0409\n",
            "  Mutual Information: 0.2990\n",
            "Scikit-learn K-Medoids Metrics:\n",
            "  Silhouette: 0.0533\n",
            "  Davies-Bouldin: 2.5113\n",
            "  Calinski-Harabasz: 6.6016\n",
            "  Diameter: 2.8825\n",
            "  Split: 2.1111\n",
            "  Adjusted Rand Index: 0.1955\n",
            "  Mutual Information: 0.3684\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "earthquake_base_labels, earthquake_sklearn_labels = kmedoids_experimentation('Earthquake', earthquake_data, alert_encoded, 4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vRq5AsrOW02b",
        "outputId": "f1155ca1-da57-487a-ccb8-640a42990c38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing Earthquake Dataset\n",
            "BASE K-Means Metrics:\n",
            "  Silhouette: 0.2183\n",
            "  Davies-Bouldin: 1.6771\n",
            "  Calinski-Harabasz: 127.3766\n",
            "  Diameter: 13.1303\n",
            "  Split: 5.8070\n",
            "  Adjusted Rand Index: 0.0837\n",
            "  Mutual Information: 0.1729\n",
            "Scikit-learn K-Medoids Metrics:\n",
            "  Silhouette: 0.2840\n",
            "  Davies-Bouldin: 1.3103\n",
            "  Calinski-Harabasz: 154.3139\n",
            "  Diameter: 13.2360\n",
            "  Split: 2.7647\n",
            "  Adjusted Rand Index: -0.0003\n",
            "  Mutual Information: 0.0872\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn_extra/cluster/_k_medoids.py:329: UserWarning: Cluster 1 is empty! self.labels_[self.medoid_indices_[1]] may not be labeled with its corresponding cluster (1).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn_extra/cluster/_k_medoids.py:329: UserWarning: Cluster 3 is empty! self.labels_[self.medoid_indices_[3]] may not be labeled with its corresponding cluster (3).\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1 Predicting alert-level based on clusters and analyzing how good the prediction is."
      ],
      "metadata": {
        "id": "o3sOIDM02c8X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "earthquake_base_labels, earthquake_sklearn_labels = kmedoids_experimentation('Earthquake', earthquake_data, alert_encoded, 4)"
      ],
      "metadata": {
        "id": "vGeHOHwu2m9d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c9cf648-ad65-428a-d19e-5dfc3f40b36c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing Earthquake Dataset\n",
            "BASE K-Means Metrics:\n",
            "  Silhouette: 0.1221\n",
            "  Davies-Bouldin: 2.3060\n",
            "  Calinski-Harabasz: 82.5489\n",
            "  Diameter: 13.3266\n",
            "  Split: 2.5607\n",
            "  Adjusted Rand Index: -0.0178\n",
            "  Mutual Information: 0.0495\n",
            "Scikit-learn K-Medoids Metrics:\n",
            "  Silhouette: 0.2840\n",
            "  Davies-Bouldin: 1.3103\n",
            "  Calinski-Harabasz: 154.3139\n",
            "  Diameter: 13.2360\n",
            "  Split: 2.7647\n",
            "  Adjusted Rand Index: -0.0003\n",
            "  Mutual Information: 0.0872\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn_extra/cluster/_k_medoids.py:329: UserWarning: Cluster 1 is empty! self.labels_[self.medoid_indices_[1]] may not be labeled with its corresponding cluster (1).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn_extra/cluster/_k_medoids.py:329: UserWarning: Cluster 3 is empty! self.labels_[self.medoid_indices_[3]] may not be labeled with its corresponding cluster (3).\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2 Reducing the number of attributes to 5 using Mutual Information Scores between feature and alert level."
      ],
      "metadata": {
        "id": "eNzmlBq72eR1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_selection import mutual_info_classif\n",
        "\n",
        "X = earthquake_data\n",
        "y = alert_encoded\n",
        "\n",
        "# Compute MI scores\n",
        "mi_scores = mutual_info_classif(X, y)\n",
        "\n",
        "# Select top 5 features\n",
        "top_5_features = np.array(np.arange(X.shape[1]))[mi_scores.argsort()[-5:]]\n",
        "\n",
        "print(\"Top 5 features most predictive of 'alert':\", earthquake_df.columns[top_5_features])\n",
        "\n",
        "X_selected = X[:, top_5_features]\n",
        "\n",
        "earthquake_data_top_5 = X_selected"
      ],
      "metadata": {
        "id": "tbt7ljcQ2mUS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13cc110d-fc53-4eb8-d8be-93c13be91842"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 features most predictive of 'alert': Index(['magnitude', 'latitude', 'longitude', 'mmi', 'sig'], dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.3 Running clustering experimentation for both base and library implementation for top-5 attribute of earthquake data"
      ],
      "metadata": {
        "id": "DXQg98-62icM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_labels, sklearn_labels = kmedoids_experimentation('Earthquake', earthquake_data, alert_encoded, 4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qIU_jztz2ljd",
        "outputId": "21670586-79b8-40bb-b43e-555b837d12d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing Earthquake Dataset\n",
            "BASE K-Means Metrics:\n",
            "  Silhouette: 0.1711\n",
            "  Davies-Bouldin: 2.1055\n",
            "  Calinski-Harabasz: 96.1199\n",
            "  Diameter: 13.3830\n",
            "  Split: 2.7010\n",
            "  Adjusted Rand Index: -0.0107\n",
            "  Mutual Information: 0.0558\n",
            "Scikit-learn K-Medoids Metrics:\n",
            "  Silhouette: 0.2840\n",
            "  Davies-Bouldin: 1.3103\n",
            "  Calinski-Harabasz: 154.3139\n",
            "  Diameter: 13.2360\n",
            "  Split: 2.7647\n",
            "  Adjusted Rand Index: -0.0003\n",
            "  Mutual Information: 0.0872\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn_extra/cluster/_k_medoids.py:329: UserWarning: Cluster 1 is empty! self.labels_[self.medoid_indices_[1]] may not be labeled with its corresponding cluster (1).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn_extra/cluster/_k_medoids.py:329: UserWarning: Cluster 3 is empty! self.labels_[self.medoid_indices_[3]] may not be labeled with its corresponding cluster (3).\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    }
  ]
}