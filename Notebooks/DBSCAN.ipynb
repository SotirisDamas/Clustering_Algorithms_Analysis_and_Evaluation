{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "COMP-6651 - Algorithm Design Techniques\n",
        "\n",
        "Project - Clustering Algorithms Analysis\n",
        "\n",
        "DBSCAN Experimentation\n",
        "\n",
        "Author - Nitheesh Kumar Kambala - 40299620\n"
      ],
      "metadata": {
        "id": "MKxpNrGRJL8f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wupD654OJHdQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2a2b8d1-26b0-4792-eaef-fefc157811ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/himanshunakrani/iris-dataset?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 0.98k/0.98k [00:00<00:00, 356kB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/katerynameleshenko/ai-index?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2.38k/2.38k [00:00<00:00, 2.81MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/shreyasur965/recent-earthquakes?dataset_version_number=3...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 214k/214k [00:00<00:00, 23.8MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n",
        "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score, adjusted_rand_score, mutual_info_score, accuracy_score\n",
        "from scipy.spatial.distance import cdist, pdist\n",
        "import os\n",
        "import kagglehub\n",
        "\n",
        "path_iris = kagglehub.dataset_download(\"himanshunakrani/iris-dataset\")\n",
        "f_path_iris = os.path.join(path_iris, 'iris.csv')\n",
        "iris_df = pd.read_csv(f_path_iris)\n",
        "iris_data_target = iris_df['species']\n",
        "iris_data_features = iris_df.drop(columns=['species']).values\n",
        "\n",
        "# Preprocessing for AI Global Index Dataset\n",
        "def preprocess_ai_index(data):\n",
        "    # Separate numerical and categorical columns\n",
        "    numeric_cols = data.select_dtypes(include=[np.number]).columns\n",
        "    categorical_cols = data.select_dtypes(exclude=[np.number]).columns\n",
        "\n",
        "    scaler = MinMaxScaler()\n",
        "    num_scaled = scaler.fit_transform(data[numeric_cols]) if len(numeric_cols) > 0 else np.array([])\n",
        "\n",
        "    encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
        "    cat_encoded = encoder.fit_transform(data[categorical_cols]) if len(categorical_cols) > 0 else np.array([])\n",
        "\n",
        "    if num_scaled.size and cat_encoded.size:\n",
        "        return np.hstack((num_scaled, cat_encoded))\n",
        "    elif num_scaled.size:\n",
        "        return num_scaled\n",
        "    else:\n",
        "        return cat_encoded\n",
        "\n",
        "path_ai_index = kagglehub.dataset_download(\"katerynameleshenko/ai-index\")\n",
        "f_path_ai_index = os.path.join(path_ai_index, 'AI_index_db.csv')\n",
        "ai_df = pd.read_csv(f_path_ai_index).dropna()\n",
        "ai_data_target = ai_df['Cluster']\n",
        "ai_data_features = ai_df.drop(columns=['Cluster'])\n",
        "ai_data_features = preprocess_ai_index(ai_data_features)\n",
        "\n",
        "# Load Global Earthquake Dataset\n",
        "path_earthquakes= kagglehub.dataset_download(\"shreyasur965/recent-earthquakes\")\n",
        "f_path_earthquakes = os.path.join(path_earthquakes, 'earthquakes.csv')\n",
        "earthquake_df = pd.read_csv(f_path_earthquakes)\n",
        "earthquake_df = earthquake_df[['magnitude', 'felt', 'cdi','mmi','tsunami','sig','depth', 'latitude', 'longitude', 'alert']].dropna()\n",
        "earthquake_data_target = earthquake_df['alert']\n",
        "earthquake_data_features = earthquake_df.drop(columns=['alert'])\n",
        "earthquake_data_features = StandardScaler().fit_transform(earthquake_data_features)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BASE_DBSCAN:\n",
        "    def __init__(self, eps, min_pts):\n",
        "        self.eps = eps\n",
        "        self.min_pts = min_pts\n",
        "\n",
        "    def fit(self, data):\n",
        "        self.data = data\n",
        "        n = len(data)\n",
        "        self.labels = np.full(n, -1)\n",
        "        cluster_id = 0\n",
        "        for i in range(n):\n",
        "            if self.labels[i] == -1:\n",
        "                neighbors = self.region_query(i)\n",
        "                if len(neighbors) >= self.min_pts:\n",
        "                    cluster_id += 1\n",
        "                    self.expand_cluster(i, neighbors, cluster_id)\n",
        "        return self.labels\n",
        "\n",
        "    def region_query(self, point_idx):\n",
        "        dists = cdist([self.data[point_idx]], self.data)[0]\n",
        "        return np.where(dists <= self.eps)[0]\n",
        "\n",
        "    def expand_cluster(self, point_idx, neighbors, cluster_id):\n",
        "        self.labels[point_idx] = cluster_id\n",
        "        i = 0\n",
        "        while i < len(neighbors):\n",
        "            neighbor_idx = neighbors[i]\n",
        "            if self.labels[neighbor_idx] == -1:\n",
        "                self.labels[neighbor_idx] = cluster_id\n",
        "            elif self.labels[neighbor_idx] == 0:\n",
        "                self.labels[neighbor_idx] = cluster_id\n",
        "                new_neighbors = self.region_query(neighbor_idx)\n",
        "                if len(new_neighbors) >= self.min_pts:\n",
        "                    neighbors = np.append(neighbors, new_neighbors)\n",
        "            i += 1"
      ],
      "metadata": {
        "id": "u5Lw1bc8Kq0d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sklearn_dbscan(data, eps, min_pts):\n",
        "    model = DBSCAN(eps=eps, min_samples=min_pts)\n",
        "    return model.fit_predict(data)"
      ],
      "metadata": {
        "id": "Pq4lf5BIK_84"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "def map_clusters_to_labels(cluster_labels, actual_labels):\n",
        "    cluster_mapping = {}\n",
        "    predicted_alerts = np.full_like(cluster_labels, fill_value=\"None\", dtype=object)\n",
        "    unique_clusters = np.unique(cluster_labels)\n",
        "    for cluster in unique_clusters:\n",
        "        if cluster == -1:\n",
        "            continue\n",
        "        cluster_indices = np.where(cluster_labels == cluster)[0]\n",
        "        cluster_alerts = actual_labels[cluster_indices]\n",
        "        most_common_alert = Counter(cluster_alerts).most_common(1)[0][0]\n",
        "        cluster_mapping[cluster] = most_common_alert\n",
        "        predicted_alerts[cluster_indices] = most_common_alert\n",
        "    return cluster_mapping, predicted_alerts\n",
        "\n",
        "def compute_clusters_mean_diameter(X, labels):\n",
        "    diameters = []\n",
        "    for cluster in np.unique(labels):\n",
        "        if cluster == -1:\n",
        "            continue\n",
        "        points = X[labels == cluster]\n",
        "        if len(points) < 2:\n",
        "            diameters.append(0)\n",
        "            continue\n",
        "        dists = pdist(points)\n",
        "        diameters.append(np.max(dists))\n",
        "    return np.mean(diameters)\n",
        "\n",
        "def compute_clusters_mean_splits(X, labels):\n",
        "    splits = []\n",
        "    for cluster in np.unique(labels):\n",
        "        if cluster == -1:\n",
        "            continue\n",
        "        in_cluster = X[labels == cluster]\n",
        "        out_cluster = X[labels != cluster]\n",
        "        if len(out_cluster) == 0:\n",
        "            splits.append(0)\n",
        "            continue\n",
        "        dists = cdist(in_cluster, out_cluster)\n",
        "        splits.append(np.min(dists))\n",
        "    return np.mean(splits)\n",
        "\n",
        "def compute_metrics(data, labels, target, predicted):\n",
        "    if len(set(labels)) > 1:\n",
        "        silhouette = silhouette_score(data, labels)\n",
        "        davies_bouldin = davies_bouldin_score(data, labels)\n",
        "        calinski_harabasz = calinski_harabasz_score(data, labels)\n",
        "        ari = adjusted_rand_score(target, predicted)\n",
        "        mi = mutual_info_score(target, predicted)\n",
        "        accuracy = accuracy_score(target, predicted)\n",
        "        mean_diameter = compute_clusters_mean_diameter(data, predicted)\n",
        "        mean_splits = compute_clusters_mean_splits(data, labels)\n",
        "    else:\n",
        "        silhouette, davies_bouldin, calinski_harabasz,ari, mi, accuracy, mean_diameter, mean_split = None, None, None, None, None, None, None, None\n",
        "\n",
        "    return {\n",
        "        'Silhouette Score': silhouette,\n",
        "        'Davies-Bouldin Index': davies_bouldin,\n",
        "        'Calinski-Harabasz Index': calinski_harabasz,\n",
        "        'ARI': ari,\n",
        "        'MI': mi,\n",
        "        'Mean Diameter': mean_diameter,\n",
        "        'Mean Splits': mean_splits,\n",
        "        'Accuracy': accuracy\n",
        "    }\n"
      ],
      "metadata": {
        "id": "37sd_aFkMHQX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dbscan_experimentation(dataset_name, features, target, min_pts, eps):\n",
        "    print(f\"\\nProcessing {dataset_name} Dataset\")\n",
        "\n",
        "    # BASE DBSCAN\n",
        "    base_labels = BASE_DBSCAN(eps, min_pts).fit(features)\n",
        "    #print(base_labels)\n",
        "    base_cluster_mapping, base_predicted = map_clusters_to_labels(base_labels, target.values)\n",
        "    base_metrics = compute_metrics(features, base_labels, target, base_predicted)\n",
        "    print(\"BASE DBSCAN Metrics:\")\n",
        "    for k, v in base_metrics.items():\n",
        "        print(f\"  {k}: {v:.4f}\")\n",
        "\n",
        "    # Scikit-learn DBSCAN\n",
        "    sklearn_labels = sklearn_dbscan(features, eps, min_pts)\n",
        "    sklearn_cluster_mapping, sklearn_predicted = map_clusters_to_labels(sklearn_labels, target.values)\n",
        "    #print(sklearn_labels)\n",
        "    sklearn_metrics = compute_metrics(features, sklearn_labels, target, sklearn_predicted)\n",
        "    print(\"Scikit-learn DBSCAN Metrics:\")\n",
        "    for k, v in sklearn_metrics.items():\n",
        "        print(f\"  {k}: {v:.4f}\")\n",
        "    return base_labels, sklearn_labels"
      ],
      "metadata": {
        "id": "_mD_QCSsMJDD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iris_base_labels, iris_sklearn_labels = dbscan_experimentation('Iris', iris_data_features, iris_data_target, 5, 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tl4b2cx35y8P",
        "outputId": "909ec6cd-6760-4b21-a274-e7a756f8f6c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing Iris Dataset\n",
            "BASE DBSCAN Metrics:\n",
            "  Silhouette Score: 0.2022\n",
            "  Davies-Bouldin Index: 0.8335\n",
            "  Calinski-Harabasz Index: 220.6209\n",
            "  ARI: 0.7860\n",
            "  MI: 0.8547\n",
            "  Mean Diameter: 2.8768\n",
            "  Mean Splits: 0.3391\n",
            "  Accuracy: 0.9200\n",
            "Scikit-learn DBSCAN Metrics:\n",
            "  Silhouette Score: 0.6864\n",
            "  Davies-Bouldin Index: 0.3836\n",
            "  Calinski-Harabasz Index: 501.9249\n",
            "  ARI: 0.5681\n",
            "  MI: 0.6365\n",
            "  Mean Diameter: 3.6342\n",
            "  Mean Splits: 1.6401\n",
            "  Accuracy: 0.6667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ai_global_base_labels, ai_global_sklearn_labels = dbscan_experimentation('AI Global Index', ai_data_features, ai_data_target, 5, 2)"
      ],
      "metadata": {
        "id": "nzbFD9lxW1f_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb090077-52ad-4ebe-b8f1-72c106f8fbf3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing AI Global Index Dataset\n",
            "BASE DBSCAN Metrics:\n",
            "  Silhouette Score: 0.1011\n",
            "  Davies-Bouldin Index: 1.8004\n",
            "  Calinski-Harabasz Index: 7.1761\n",
            "  ARI: 0.0420\n",
            "  MI: 0.1517\n",
            "  Mean Diameter: 2.5514\n",
            "  Mean Splits: 2.0063\n",
            "  Accuracy: 0.3226\n",
            "Scikit-learn DBSCAN Metrics:\n",
            "  Silhouette Score: 0.1011\n",
            "  Davies-Bouldin Index: 1.8004\n",
            "  Calinski-Harabasz Index: 7.1761\n",
            "  ARI: 0.0420\n",
            "  MI: 0.1517\n",
            "  Mean Diameter: 2.5514\n",
            "  Mean Splits: 2.0063\n",
            "  Accuracy: 0.3226\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "earthquake_base_labels, earthquake_sklearn_labels = dbscan_experimentation('Earthquake', earthquake_data_features, earthquake_data_target, 5, 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Me9quxyd52mV",
        "outputId": "3f804b64-c0d0-4f6a-deac-25a67e967e44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing Earthquake Dataset\n",
            "BASE DBSCAN Metrics:\n",
            "  Silhouette Score: 0.0953\n",
            "  Davies-Bouldin Index: 1.6416\n",
            "  Calinski-Harabasz Index: 13.0675\n",
            "  ARI: 0.3460\n",
            "  MI: 0.1290\n",
            "  Mean Diameter: 9.6661\n",
            "  Mean Splits: 0.6016\n",
            "  Accuracy: 0.8233\n",
            "Scikit-learn DBSCAN Metrics:\n",
            "  Silhouette Score: 0.1251\n",
            "  Davies-Bouldin Index: 1.6527\n",
            "  Calinski-Harabasz Index: 39.3070\n",
            "  ARI: 0.2702\n",
            "  MI: 0.0854\n",
            "  Mean Diameter: 12.0946\n",
            "  Mean Splits: 0.9709\n",
            "  Accuracy: 0.8233\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1 Predicting alert-level based on clusters and analyzing how good the prediction is."
      ],
      "metadata": {
        "id": "AklTRyg1GKiC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "earthquake_base_labels, earthquake_sklearn_labels = dbscan_experimentation('Earthquake', earthquake_data_features, earthquake_data_target, 5, 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D9RwlFNrJFCV",
        "outputId": "a2b56b26-fb46-41ba-93bd-8aacbd57c831"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing Earthquake Dataset\n",
            "BASE DBSCAN Metrics:\n",
            "  Silhouette Score: 0.0953\n",
            "  Davies-Bouldin Index: 1.6416\n",
            "  Calinski-Harabasz Index: 13.0675\n",
            "  ARI: 0.3460\n",
            "  MI: 0.1290\n",
            "  Mean Diameter: 9.6661\n",
            "  Mean Splits: 0.6016\n",
            "  Accuracy: 0.8233\n",
            "Scikit-learn DBSCAN Metrics:\n",
            "  Silhouette Score: 0.1251\n",
            "  Davies-Bouldin Index: 1.6527\n",
            "  Calinski-Harabasz Index: 39.3070\n",
            "  ARI: 0.2702\n",
            "  MI: 0.0854\n",
            "  Mean Diameter: 12.0946\n",
            "  Mean Splits: 0.9709\n",
            "  Accuracy: 0.8233\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2 Reducing the number of attributes to 5 using Mutual Information Scores between feature and alert level."
      ],
      "metadata": {
        "id": "s1jb94TEt_KD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_selection import mutual_info_classif\n",
        "\n",
        "X = earthquake_data_features\n",
        "y = earthquake_data_target\n",
        "\n",
        "# Compute MI scores\n",
        "mi_scores = mutual_info_classif(X, y)\n",
        "\n",
        "# Select top 5 features\n",
        "top_5_features = np.array(np.arange(X.shape[1]))[mi_scores.argsort()[-5:]]\n",
        "\n",
        "print(\"Top 5 features most predictive of 'alert':\", earthquake_df.columns[top_5_features])\n",
        "\n",
        "X_selected = X[:, top_5_features]\n",
        "\n",
        "earthquake_data_top_5 = X_selected"
      ],
      "metadata": {
        "id": "Nw_JSJiLt-cp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a90c5dc-7891-4fa0-af37-6a8e9e0b4078"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 features most predictive of 'alert': Index(['magnitude', 'latitude', 'longitude', 'mmi', 'sig'], dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.3 Running clustering experimentation for both base and library implementation for top-5 attribute of earthquake data"
      ],
      "metadata": {
        "id": "DIPnYxytvPXz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_labels, sklearn_labels = dbscan_experimentation('Earthquake', earthquake_data_top_5, earthquake_data_target, 3, 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7h16v4hDIOVf",
        "outputId": "6a1b9f15-aea0-4183-ed80-35d00e64e466"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing Earthquake Dataset\n",
            "BASE DBSCAN Metrics:\n",
            "  Silhouette Score: 0.2500\n",
            "  Davies-Bouldin Index: 1.2174\n",
            "  Calinski-Harabasz Index: 108.2232\n",
            "  ARI: 0.6649\n",
            "  MI: 0.1728\n",
            "  Mean Diameter: 3.8201\n",
            "  Mean Splits: 0.6227\n",
            "  Accuracy: 0.9490\n",
            "Scikit-learn DBSCAN Metrics:\n",
            "  Silhouette Score: 0.1018\n",
            "  Davies-Bouldin Index: 1.1700\n",
            "  Calinski-Harabasz Index: 60.3764\n",
            "  ARI: 0.5611\n",
            "  MI: 0.1397\n",
            "  Mean Diameter: 3.7742\n",
            "  Mean Splits: 1.2190\n",
            "  Accuracy: 0.9411\n"
          ]
        }
      ]
    }
  ]
}