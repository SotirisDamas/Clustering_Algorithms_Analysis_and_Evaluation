{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder, LabelEncoder\n",
        "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score, adjusted_rand_score, mutual_info_score, accuracy_score, normalized_mutual_info_score\n",
        "from scipy.spatial.distance import pdist, squareform, cdist\n",
        "import os\n",
        "import kagglehub"
      ],
      "metadata": {
        "id": "EAsk5FGGYqLc"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "j9cYhJi6Hs2m"
      },
      "outputs": [],
      "source": [
        "path_iris = kagglehub.dataset_download(\"himanshunakrani/iris-dataset\")\n",
        "f_path_iris = os.path.join(path_iris, 'iris.csv')\n",
        "iris_df = pd.read_csv(f_path_iris)\n",
        "# Extract labels (species)\n",
        "iris_labels = iris_df['species']\n",
        "# Remove label from the features\n",
        "iris_features = iris_df.drop(columns=['species'])\n",
        "# Convert to numpy\n",
        "iris_data = iris_features.values\n",
        "\n",
        "\n",
        "# Preprocessing for AI Global Index Dataset\n",
        "def preprocess_ai_index(data):\n",
        "    # Separate numerical and categorical columns\n",
        "    numeric_cols = data.select_dtypes(include=[np.number]).columns\n",
        "    categorical_cols = data.select_dtypes(exclude=[np.number]).columns\n",
        "\n",
        "    scaler = MinMaxScaler()\n",
        "    num_scaled = scaler.fit_transform(data[numeric_cols]) if len(numeric_cols) > 0 else np.array([])\n",
        "\n",
        "    encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
        "    cat_encoded = encoder.fit_transform(data[categorical_cols]) if len(categorical_cols) > 0 else np.array([])\n",
        "\n",
        "    if num_scaled.size and cat_encoded.size:\n",
        "        return np.hstack((num_scaled, cat_encoded))\n",
        "    elif num_scaled.size:\n",
        "        return num_scaled\n",
        "    else:\n",
        "        return cat_encoded\n",
        "\n",
        "path_ai_index = kagglehub.dataset_download(\"katerynameleshenko/ai-index\")\n",
        "f_path_ai_index = os.path.join(path_ai_index, 'AI_index_db.csv')\n",
        "ai_df = pd.read_csv(f_path_ai_index)\n",
        "ai_df = ai_df.dropna()\n",
        "# Extract the 'Cluster' column as the label\n",
        "ai_labels = ai_df['Cluster']\n",
        "# Remove the 'Cluster' column from the features\n",
        "ai_df_features = ai_df.drop(columns=['Cluster'])\n",
        "# Preprocess the remaining features\n",
        "ai_data = preprocess_ai_index(ai_df_features)\n",
        "\n",
        "path_earthquakes= kagglehub.dataset_download(\"shreyasur965/recent-earthquakes\")\n",
        "f_path_earthquakes = os.path.join(path_earthquakes, 'earthquakes.csv')\n",
        "earthquake_df = pd.read_csv(f_path_earthquakes)\n",
        "earthquake_df = earthquake_df[['magnitude', 'felt', 'cdi','mmi','tsunami','sig','depth', 'latitude', 'longitude', 'alert']].dropna()\n",
        "# Extract the alert labels\n",
        "earthquake_data_alerts = earthquake_df['alert']\n",
        "alert_encoded = LabelEncoder().fit_transform(earthquake_data_alerts)\n",
        "# Remove the label from features\n",
        "earthquake_data_features = earthquake_df.drop(columns=['alert'])\n",
        "# Scale the numeric features\n",
        "earthquake_data = StandardScaler().fit_transform(earthquake_data_features)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BASE_KMEANS:\n",
        "    def __init__(self, k, max_iters=100, tolerance=1e-4):\n",
        "        self.k = k\n",
        "        self.max_iters = max_iters\n",
        "        self.tolerance = tolerance\n",
        "        self.centroids = None\n",
        "        self.labels = None\n",
        "\n",
        "    def fit(self, data):\n",
        "        self.data = data\n",
        "        n_samples, n_features = data.shape\n",
        "        random_indices = np.random.choice(n_samples, self.k, replace=False)\n",
        "        self.centroids = data[random_indices]\n",
        "        prev_centroids = np.zeros_like(self.centroids)\n",
        "        self.labels = np.zeros(n_samples, dtype=int)  # Ensure labels are integers\n",
        "\n",
        "        for _ in range(self.max_iters):\n",
        "            distances = np.linalg.norm(data[:, np.newaxis] - self.centroids, axis=2)\n",
        "            self.labels = np.argmin(distances, axis=1)\n",
        "            for i in range(self.k):\n",
        "                cluster_points = data[self.labels == i]\n",
        "                if len(cluster_points) > 0:\n",
        "                    self.centroids[i] = np.mean(cluster_points, axis=0)\n",
        "            if np.linalg.norm(self.centroids - prev_centroids) < self.tolerance:\n",
        "                break\n",
        "            prev_centroids = self.centroids.copy()\n",
        "        return self.labels, self.centroids"
      ],
      "metadata": {
        "id": "nbBtrbr8fkaH"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sklearn_kmeans(features, n_clusters):\n",
        "    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
        "    labels = kmeans.fit_predict(features)\n",
        "    return labels"
      ],
      "metadata": {
        "id": "AJac10hbflz1"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_diameter(X, labels):\n",
        "    \"\"\"\n",
        "    Computes the average 'diameter' across all clusters,\n",
        "    where 'diameter' of a cluster is the maximum distance\n",
        "    between any two points in that cluster.\n",
        "    \"\"\"\n",
        "    unique_labels = np.unique(labels)\n",
        "    diameters = []\n",
        "    for lbl in unique_labels:\n",
        "        cluster_points = X[labels == lbl]\n",
        "        if len(cluster_points) > 1:\n",
        "            # pairwise distances in the cluster\n",
        "            dist_matrix = squareform(pdist(cluster_points))\n",
        "            diameters.append(dist_matrix.max())\n",
        "        else:\n",
        "            # A single point has diameter 0\n",
        "            diameters.append(0.0)\n",
        "    return np.mean(diameters)\n",
        "\n",
        "\n",
        "def compute_split(X, labels):\n",
        "    \"\"\"\n",
        "    An example 'split' metric: ratio of the size of the largest cluster\n",
        "    to the size of the smallest cluster. If there's only one cluster, return 1.\n",
        "    \"\"\"\n",
        "    unique_labels, counts = np.unique(labels, return_counts=True)\n",
        "    if len(counts) < 2:\n",
        "        return 1.0\n",
        "    return counts.max() / counts.min()\n",
        "\n",
        "\n",
        "def evaluate_clustering(X, labels, true_labels=None):\n",
        "    \"\"\"\n",
        "    Compute a set of metrics for the given clustering labels.\n",
        "    Some metrics require ground truth (true_labels).\n",
        "    If no true_labels is provided, ARI and MI will be omitted.\n",
        "    Returns a dict of metric_name -> value.\n",
        "    \"\"\"\n",
        "    metrics_dict = {}\n",
        "\n",
        "    # Unsupervised metrics\n",
        "    metrics_dict[\"Silhouette\"] = silhouette_score(X, labels)\n",
        "    metrics_dict[\"Davies-Bouldin\"] = davies_bouldin_score(X, labels)\n",
        "    metrics_dict[\"Calinski-Harabasz\"] = calinski_harabasz_score(X, labels)\n",
        "    metrics_dict[\"Diameter\"] = compute_diameter(X, labels)\n",
        "    metrics_dict[\"Split\"] = compute_split(X, labels)\n",
        "\n",
        "    # If we have ground-truth labels, we can compute supervised metrics\n",
        "    if true_labels is not None:\n",
        "        metrics_dict[\"Adjusted Rand Index\"] = adjusted_rand_score(true_labels, labels)\n",
        "        # Using normalized_mutual_info_score as a measure of MI\n",
        "        metrics_dict[\"Mutual Information\"] = normalized_mutual_info_score(true_labels, labels)\n",
        "    return metrics_dict"
      ],
      "metadata": {
        "id": "PBBXoirXfnX8"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def kmeans_experimentation(dataset_name, features, target, k):\n",
        "    print(f\"\\nProcessing {dataset_name} Dataset\")\n",
        "\n",
        "    # Get labels from your BASE_KMEANS\n",
        "    base_labels, _ = BASE_KMEANS(k).fit(features)\n",
        "\n",
        "    # Evaluate your BASE_KMEANS using the evaluate_clustering function\n",
        "    # base_metrics = evaluate_clustering(features, base_labels, true_labels=target.values)\n",
        "    base_metrics = evaluate_clustering(features, base_labels, true_labels=target)\n",
        "    print(\"BASE K-Means Metrics:\")\n",
        "    for t, v in base_metrics.items():\n",
        "        print(f\"  {t}: {v:.4f}\")\n",
        "\n",
        "    # Get labels from scikit-learn's KMeans\n",
        "    sklearn_labels = sklearn_kmeans(features, k)\n",
        "\n",
        "    # Evaluate scikit-learn's KMeans using the evaluate_clustering function\n",
        "    # sklearn_metrics = evaluate_clustering(features, sklearn_labels, true_labels=target.values)\n",
        "    sklearn_metrics = evaluate_clustering(features, sklearn_labels, true_labels=target)\n",
        "    print(\"\\nScikit-learn K-Means Metrics:\")\n",
        "    for t, v in sklearn_metrics.items():\n",
        "        print(f\"  {t}: {v:.4f}\")\n",
        "\n",
        "    return base_labels, sklearn_labels"
      ],
      "metadata": {
        "id": "WCndlhHVfoHy"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_labels_iris, sklearn_labels_iris = kmeans_experimentation(\"Iris\", iris_data, iris_labels, 3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "OS_NFZ5LfpfT",
        "outputId": "fe19f97b-4ce8-48ca-e8b9-ed59c1483f03"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing Iris Dataset\n",
            "BASE K-Means Metrics:\n",
            "  Silhouette: 0.5510\n",
            "  Davies-Bouldin: 0.6664\n",
            "  Calinski-Harabasz: 560.3660\n",
            "  Diameter: 2.5210\n",
            "  Split: 1.5641\n",
            "  Adjusted Rand Index: 0.7163\n",
            "  Mutual Information: 0.7419\n",
            "\n",
            "Scikit-learn K-Means Metrics:\n",
            "  Silhouette: 0.5526\n",
            "  Davies-Bouldin: 0.6623\n",
            "  Calinski-Harabasz: 560.3999\n",
            "  Diameter: 2.5085\n",
            "  Split: 1.6316\n",
            "  Adjusted Rand Index: 0.7302\n",
            "  Mutual Information: 0.7582\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ai_global_base_labels, ai_global_sklearn_labels = kmeans_experimentation('AI Global Index', ai_data, ai_labels, 5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "OeCj3zuWfrkb",
        "outputId": "096e1cff-7346-4ece-ba76-a9d1290055f8"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing AI Global Index Dataset\n",
            "BASE K-Means Metrics:\n",
            "  Silhouette: 0.0589\n",
            "  Davies-Bouldin: 2.3240\n",
            "  Calinski-Harabasz: 7.6086\n",
            "  Diameter: 2.4678\n",
            "  Split: 2.4286\n",
            "  Adjusted Rand Index: 0.0529\n",
            "  Mutual Information: 0.2908\n",
            "\n",
            "Scikit-learn K-Means Metrics:\n",
            "  Silhouette: 0.1848\n",
            "  Davies-Bouldin: 1.6109\n",
            "  Calinski-Harabasz: 10.1389\n",
            "  Diameter: 2.7197\n",
            "  Split: 5.4000\n",
            "  Adjusted Rand Index: 0.0016\n",
            "  Mutual Information: 0.3006\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "earthquake_base_labels, earthquake_sklearn_labels = kmeans_experimentation('Earthquake', earthquake_data, alert_encoded, 4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "yknXvzq_fs07",
        "outputId": "3ac3982f-6b18-4663-f975-1db4607b6ad5"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing Earthquake Dataset\n",
            "BASE K-Means Metrics:\n",
            "  Silhouette: 0.2978\n",
            "  Davies-Bouldin: 1.2812\n",
            "  Calinski-Harabasz: 158.7361\n",
            "  Diameter: 13.1146\n",
            "  Split: 2.9252\n",
            "  Adjusted Rand Index: 0.0103\n",
            "  Mutual Information: 0.0697\n",
            "\n",
            "Scikit-learn K-Means Metrics:\n",
            "  Silhouette: 0.3498\n",
            "  Davies-Bouldin: 1.2806\n",
            "  Calinski-Harabasz: 162.9608\n",
            "  Diameter: 12.6598\n",
            "  Split: 8.1930\n",
            "  Adjusted Rand Index: 0.1056\n",
            "  Mutual Information: 0.1820\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1 Predicting alert-level based on clusters and analyzing how good the prediction is."
      ],
      "metadata": {
        "id": "825K5cT1fw-8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "earthquake_base_labels, earthquake_sklearn_labels = kmeans_experimentation('Earthquake', earthquake_data, alert_encoded, 3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "IIUHAuGifuWL",
        "outputId": "2bcce33c-620f-48c5-8325-b6c57bd7c791"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing Earthquake Dataset\n",
            "BASE K-Means Metrics:\n",
            "  Silhouette: 0.3372\n",
            "  Davies-Bouldin: 1.4329\n",
            "  Calinski-Harabasz: 134.0585\n",
            "  Diameter: 14.2742\n",
            "  Split: 9.5385\n",
            "  Adjusted Rand Index: 0.2854\n",
            "  Mutual Information: 0.2385\n",
            "\n",
            "Scikit-learn K-Means Metrics:\n",
            "  Silhouette: 0.2626\n",
            "  Davies-Bouldin: 1.5092\n",
            "  Calinski-Harabasz: 151.4945\n",
            "  Diameter: 15.1691\n",
            "  Split: 3.0556\n",
            "  Adjusted Rand Index: 0.0439\n",
            "  Mutual Information: 0.1248\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2 Reducing the number of attributes to 5 using Mutual Information Scores between feature and alert level."
      ],
      "metadata": {
        "id": "ucer86Brf0d2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_selection import mutual_info_classif\n",
        "\n",
        "X = earthquake_data\n",
        "y = alert_encoded\n",
        "\n",
        "# Compute MI scores\n",
        "mi_scores = mutual_info_classif(X, y)\n",
        "\n",
        "# Select top 5 features\n",
        "top_5_features = np.array(np.arange(X.shape[1]))[mi_scores.argsort()[-5:]]\n",
        "\n",
        "print(\"Top 5 features most predictive of 'alert':\", earthquake_df.columns[top_5_features])\n",
        "\n",
        "X_selected = X[:, top_5_features]\n",
        "\n",
        "earthquake_data_top_5 = X_selected"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "-tK8pF-Kf2hZ",
        "outputId": "2f394137-5232-4f92-bb72-ffc043bc35b5"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 features most predictive of 'alert': Index(['magnitude', 'latitude', 'longitude', 'mmi', 'sig'], dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.3 Running clustering experimentation for both base and library implementation for top-5 attribute of earthquake data"
      ],
      "metadata": {
        "id": "sf0LTYrSf4SK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_labels, sklearn_labels = kmeans_experimentation('Earthquake', earthquake_data, alert_encoded, 3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "nRtL0W3mf6xE",
        "outputId": "f2663866-0392-4057-f305-7506c7c2ccdb"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing Earthquake Dataset\n",
            "BASE K-Means Metrics:\n",
            "  Silhouette: 0.3157\n",
            "  Davies-Bouldin: 1.4366\n",
            "  Calinski-Harabasz: 147.1532\n",
            "  Diameter: 15.0971\n",
            "  Split: 5.7849\n",
            "  Adjusted Rand Index: 0.1795\n",
            "  Mutual Information: 0.1857\n",
            "\n",
            "Scikit-learn K-Means Metrics:\n",
            "  Silhouette: 0.2626\n",
            "  Davies-Bouldin: 1.5092\n",
            "  Calinski-Harabasz: 151.4945\n",
            "  Diameter: 15.1691\n",
            "  Split: 3.0556\n",
            "  Adjusted Rand Index: 0.0439\n",
            "  Mutual Information: 0.1248\n"
          ]
        }
      ]
    }
  ]
}