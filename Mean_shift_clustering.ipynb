{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "COMP-6651 - Algorithm Design Techniques\n",
        "\n",
        "Project - Clustering Algorithms Analysis\n",
        "\n"
      ],
      "metadata": {
        "id": "SfI45T2-BGMv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import MeanShift as SklearnMeanShift\n",
        "from sklearn.metrics.pairwise import pairwise_distances\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder, LabelEncoder\n",
        "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
        "import os\n",
        "import kagglehub\n",
        "from sklearn.feature_selection import mutual_info_classif\n",
        "from sklearn.metrics import (calinski_harabasz_score, silhouette_score,\n",
        "                             davies_bouldin_score, adjusted_rand_score,\n",
        "                             normalized_mutual_info_score)"
      ],
      "metadata": {
        "id": "DkP_hTZO7hpf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_ai_index(data):\n",
        "    # Separate numerical and categorical columns\n",
        "    numeric_cols = data.select_dtypes(include=[np.number]).columns\n",
        "    categorical_cols = data.select_dtypes(exclude=[np.number]).columns\n",
        "\n",
        "    scaler = MinMaxScaler()\n",
        "    num_scaled = scaler.fit_transform(data[numeric_cols]) if len(numeric_cols) > 0 else np.array([])\n",
        "\n",
        "    encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
        "    cat_encoded = encoder.fit_transform(data[categorical_cols]) if len(categorical_cols) > 0 else np.array([])\n",
        "\n",
        "    if num_scaled.size and cat_encoded.size:\n",
        "        return np.hstack((num_scaled, cat_encoded))\n",
        "    elif num_scaled.size:\n",
        "        return num_scaled\n",
        "    else:\n",
        "        return cat_encoded\n"
      ],
      "metadata": {
        "id": "rlloS6VIkfvA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path_iris = kagglehub.dataset_download(\"himanshunakrani/iris-dataset\")\n",
        "f_path_iris = os.path.join(path_iris, 'iris.csv')\n",
        "iris_df = pd.read_csv(f_path_iris)\n",
        "# Extract labels (species)\n",
        "iris_labels = iris_df['species']\n",
        "# Remove label from the features\n",
        "iris_features = iris_df.drop(columns=['species'])\n",
        "# Convert to numpy\n",
        "iris_data = iris_features.values\n",
        "\n",
        "\n",
        "\n",
        "path_ai_index = kagglehub.dataset_download(\"katerynameleshenko/ai-index\")\n",
        "f_path_ai_index = os.path.join(path_ai_index, 'AI_index_db.csv')\n",
        "ai_df = pd.read_csv(f_path_ai_index)\n",
        "ai_df = ai_df.dropna()\n",
        "# Extract the 'Cluster' column as the label\n",
        "ai_labels = ai_df['Cluster']\n",
        "# Remove the 'Cluster' column from the features\n",
        "ai_df_features = ai_df.drop(columns=['Cluster'])\n",
        "# Preprocess the remaining features\n",
        "ai_data = preprocess_ai_index(ai_df_features)\n",
        "\n",
        "path_earthquakes= kagglehub.dataset_download(\"shreyasur965/recent-earthquakes\")\n",
        "f_path_earthquakes = os.path.join(path_earthquakes, 'earthquakes.csv')\n",
        "earthquake_df = pd.read_csv(f_path_earthquakes)\n",
        "earthquake_df = earthquake_df[['magnitude', 'felt', 'cdi','mmi','tsunami','sig','depth', 'latitude', 'longitude', 'alert']].dropna()\n",
        "# Extract the alert labels\n",
        "earthquake_data_alerts = earthquake_df['alert']\n",
        "alert_encoded = LabelEncoder().fit_transform(earthquake_data_alerts)\n",
        "# Remove the label from features\n",
        "earthquake_data_features = earthquake_df.drop(columns=['alert'])\n",
        "# Scale the numeric features\n",
        "earthquake_data = StandardScaler().fit_transform(earthquake_data_features)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2RwC-u4kjOs",
        "outputId": "32c4a664-4748-4f01-f330-705ee819f7b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/himanshunakrani/iris-dataset?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 0.98k/0.98k [00:00<00:00, 1.82MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/katerynameleshenko/ai-index?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2.38k/2.38k [00:00<00:00, 3.64MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/shreyasur965/recent-earthquakes?dataset_version_number=3...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 214k/214k [00:00<00:00, 43.5MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import pairwise_distances\n",
        "\n",
        "class MeanShift:\n",
        "    def __init__(self, bandwidth=0.5, convergence_thresh=1e-3, max_iter=100):\n",
        "        self.bandwidth = bandwidth  # Renamed from sigma to match conventions\n",
        "        self.convergence_thresh = convergence_thresh  # Renamed from epsilon\n",
        "        self.max_iter = max_iter\n",
        "        self.labels_ = None\n",
        "        self.cluster_centers_ = None\n",
        "\n",
        "    def fit(self, X):\n",
        "        X = np.asarray(X)\n",
        "        n_samples, _ = X.shape\n",
        "        modes = np.zeros_like(X)\n",
        "\n",
        "        # Step 1: Compute modes for each point\n",
        "        for i in range(n_samples):\n",
        "            x_old = X[i].copy()\n",
        "            for _ in range(self.max_iter):\n",
        "                # Compute weights using Gaussian kernel\n",
        "                distances = np.sum((X - x_old) ** 2, axis=1)\n",
        "                weights = np.exp(-distances / (2 * self.bandwidth ** 2))\n",
        "                weights_sum = np.sum(weights)\n",
        "\n",
        "                if weights_sum < 1e-10:  # Handle zero division\n",
        "                    break\n",
        "\n",
        "                # Update mean-shift vector\n",
        "                x_new = np.dot(weights, X) / weights_sum\n",
        "\n",
        "                # Check convergence\n",
        "                if np.linalg.norm(x_new - x_old) < self.convergence_thresh:\n",
        "                    break\n",
        "                x_old = x_new\n",
        "            modes[i] = x_old\n",
        "\n",
        "        # Step 2: Merge modes using adaptive epsilon\n",
        "        self._auto_merge_modes(modes)\n",
        "        return self\n",
        "\n",
        "    def _auto_merge_modes(self, modes):\n",
        "        \"\"\"Automatically determine merge threshold and cluster modes\"\"\"\n",
        "        # Compute pairwise distances between modes\n",
        "        mode_distances = pairwise_distances(modes)\n",
        "\n",
        "        # Use 10th percentile of non-zero distances as epsilon\n",
        "        non_zero_dists = mode_distances[mode_distances > 0]\n",
        "        if len(non_zero_dists) == 0:\n",
        "            self.epsilon = 0.0\n",
        "        else:\n",
        "            self.epsilon = np.percentile(non_zero_dists, 20)  # 5th percentile\n",
        "\n",
        "        # Apply connected components\n",
        "        self.labels_ = self._connected_components(modes, self.epsilon)\n",
        "\n",
        "        # Extract unique cluster centers\n",
        "        unique_labels = np.unique(self.labels_)\n",
        "        self.cluster_centers_ = np.array([\n",
        "            modes[self.labels_ == label][0]\n",
        "            for label in unique_labels\n",
        "        ])\n",
        "\n",
        "    def _connected_components(self, modes, epsilon):\n",
        "        \"\"\"Optimized union-find with path compression\"\"\"\n",
        "        n = len(modes)\n",
        "        parent = list(range(n))\n",
        "\n",
        "        # Precompute all pairs within epsilon\n",
        "        dist_matrix = pairwise_distances(modes)\n",
        "        rows, cols = np.where((dist_matrix <= epsilon) & (np.eye(n) == 0))\n",
        "\n",
        "        # Union-Find operations\n",
        "        def find(u):\n",
        "            while parent[u] != u:\n",
        "                parent[u] = parent[parent[u]]  # Path compression\n",
        "                u = parent[u]\n",
        "            return u\n",
        "\n",
        "        for i, j in zip(rows, cols):\n",
        "            if i < j:\n",
        "                root_i = find(i)\n",
        "                root_j = find(j)\n",
        "                if root_i != root_j:\n",
        "                    parent[root_j] = root_i\n",
        "\n",
        "        # Label clusters\n",
        "        labels = np.empty(n, dtype=int)\n",
        "        label_map = {}\n",
        "        current_label = 0\n",
        "        for i in range(n):\n",
        "            root = find(i)\n",
        "            if root not in label_map:\n",
        "                label_map[root] = current_label\n",
        "                current_label += 1\n",
        "            labels[i] = label_map[root]\n",
        "\n",
        "        return labels"
      ],
      "metadata": {
        "id": "W8RGRvvHm74C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_diameter(X, labels):\n",
        "    \"\"\"\n",
        "    Computes the average 'diameter' across all clusters,\n",
        "    where 'diameter' of a cluster is the maximum distance\n",
        "    between any two points in that cluster.\n",
        "    \"\"\"\n",
        "    from scipy.spatial.distance import pdist, squareform\n",
        "\n",
        "    unique_labels = np.unique(labels)\n",
        "    diameters = []\n",
        "    for lbl in unique_labels:\n",
        "        cluster_points = X[labels == lbl]\n",
        "        if len(cluster_points) > 1:\n",
        "            # pairwise distances in the cluster\n",
        "            dist_matrix = squareform(pdist(cluster_points))\n",
        "            diameters.append(dist_matrix.max())\n",
        "        else:\n",
        "            # A single point has diameter 0\n",
        "            diameters.append(0.0)\n",
        "    return np.mean(diameters)\n",
        "\n",
        "\n",
        "def compute_split(X, labels):\n",
        "    \"\"\"\n",
        "    An example 'split' metric: ratio of the size of the largest cluster\n",
        "    to the size of the smallest cluster. If there's only one cluster, return 1.\n",
        "    \"\"\"\n",
        "    unique_labels, counts = np.unique(labels, return_counts=True)\n",
        "    if len(counts) < 2:\n",
        "        return 1.0\n",
        "    return counts.max() / counts.min()\n",
        "\n",
        "\n",
        "def evaluate_clustering(X, labels, true_labels=None):\n",
        "    \"\"\"\n",
        "    Compute a set of metrics for the given clustering labels.\n",
        "    Some metrics require ground truth (true_labels).\n",
        "    If no true_labels is provided, ARI and MI will be omitted.\n",
        "    Returns a dict of metric_name -> value.\n",
        "    \"\"\"\n",
        "    metrics_dict = {}\n",
        "\n",
        "    # Unsupervised metrics\n",
        "    metrics_dict[\"Silhouette\"] = silhouette_score(X, labels)\n",
        "    metrics_dict[\"Davies-Bouldin\"] = davies_bouldin_score(X, labels)\n",
        "    metrics_dict[\"Calinski-Harabasz\"] = calinski_harabasz_score(X, labels)\n",
        "    metrics_dict[\"Diameter\"] = compute_diameter(X, labels)\n",
        "    metrics_dict[\"Split\"] = compute_split(X, labels)\n",
        "\n",
        "    # If we have ground-truth labels, we can compute supervised metrics\n",
        "    if true_labels is not None:\n",
        "        metrics_dict[\"Adjusted Rand Index\"] = adjusted_rand_score(true_labels, labels)\n",
        "        # Using normalized_mutual_info_score as a measure of MI\n",
        "        metrics_dict[\"Mutual Information\"] = normalized_mutual_info_score(true_labels, labels)\n",
        "\n",
        "    return metrics_dict\n"
      ],
      "metadata": {
        "id": "eC6HKhVPnLUp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def run_experiment(name, X, bandwidth=None, max_iter=300, convergence_thresh=1e-3,\n",
        "                             true_labels=None):\n",
        "    print(f\"----- {name} -----\")\n",
        "\n",
        "    # Custom Mean Shift Implementation\n",
        "    custom_ms = MeanShift(\n",
        "        bandwidth=bandwidth,\n",
        "        max_iter=max_iter\n",
        "    )\n",
        "    custom_ms.fit(X)\n",
        "    custom_labels = custom_ms.labels_\n",
        "    n_custom_clusters = len(custom_ms.cluster_centers_)\n",
        "    print(f\"Custom MS: Number of clusters = {n_custom_clusters}\")\n",
        "\n",
        "    # Scikit-learn's Implementation\n",
        "    sklearn_ms = SklearnMeanShift(\n",
        "        bandwidth=bandwidth,\n",
        "        max_iter=max_iter,\n",
        "        bin_seeding=False,\n",
        "        min_bin_freq=1,\n",
        "        cluster_all=True\n",
        "    )\n",
        "    sklearn_ms.fit(X)\n",
        "    sklearn_labels = sklearn_ms.labels_\n",
        "    n_sklearn_clusters = len(sklearn_ms.cluster_centers_)\n",
        "    print(f\"Sklearn MS: Number of clusters = {n_sklearn_clusters}\")\n",
        "\n",
        "\n",
        "    custom_metrics = evaluate_clustering(X, custom_labels, true_labels=true_labels)\n",
        "    sklearn_metrics = evaluate_clustering(X, sklearn_labels, true_labels=true_labels)\n",
        "\n",
        "    print(\"\\nCustom MS Metrics:\")\n",
        "    for k, v in custom_metrics.items():\n",
        "        print(f\"  {k}: {v:.4f}\")\n",
        "\n",
        "    print(\"\\nSklearn MS Metrics:\")\n",
        "    for k, v in sklearn_metrics.items():\n",
        "        print(f\"  {k}: {v:.4f}\")\n",
        "\n",
        "    print(\"-\" * 40)\n",
        "    return custom_labels, sklearn_labels"
      ],
      "metadata": {
        "id": "va_dcAXonbO6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import estimate_bandwidth\n",
        "bandwidth = estimate_bandwidth(iris_data, quantile=0.4)\n",
        "\n",
        "\n",
        "print(\"Running Custom and Sklearn MS on Iris Dataset\")\n",
        "custom_labels_iris, sklearn_labels_iris = run_experiment(\n",
        "    \"Iris\", iris_data,\n",
        "    bandwidth=bandwidth,  # Controls cluster granularity\n",
        "    true_labels=iris_labels\n",
        ")\n",
        "bandwidth = estimate_bandwidth(ai_data, quantile=0.4)\n",
        "\n",
        "print(\"\\nRunning Custom and Sklearn MS on AI Global Index Dataset\")\n",
        "custom_labels_ai, sklearn_labels_ai = run_experiment(\n",
        "    \"AI Global Index\", ai_data,\n",
        "    bandwidth=bandwidth,  # Larger bandwidth for broader clusters\n",
        "    true_labels=ai_labels\n",
        ")\n",
        "bandwidth = estimate_bandwidth(earthquake_data, quantile=0.4)\n",
        "\n",
        "print(\"\\nRunning Custom and Sklearn MS on Earthquake Dataset\")\n",
        "custom_labels_eq, sklearn_labels_eq = run_experiment(\n",
        "    \"Earthquake\", earthquake_data,\n",
        "    bandwidth=bandwidth,  # Smaller bandwidth for fine-grained clusters\n",
        "    true_labels=alert_encoded\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kjqoz2-in4E7",
        "outputId": "8d920c5f-a0ff-4dd0-fb23-e5ad465469d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Custom and Sklearn MS on Iris Dataset\n",
            "----- Iris -----\n",
            "Custom MS: Number of clusters = 2\n",
            "Sklearn MS: Number of clusters = 2\n",
            "\n",
            "Custom MS Metrics:\n",
            "  Silhouette: 0.5025\n",
            "  Davies-Bouldin: 0.6699\n",
            "  Calinski-Harabasz: 266.7661\n",
            "  Diameter: 3.6179\n",
            "  Split: 1.1429\n",
            "  Adjusted Rand Index: 0.4240\n",
            "  Mutual Information: 0.4822\n",
            "\n",
            "Sklearn MS Metrics:\n",
            "  Silhouette: 0.6855\n",
            "  Davies-Bouldin: 0.3893\n",
            "  Calinski-Harabasz: 508.8822\n",
            "  Diameter: 3.6903\n",
            "  Split: 1.9412\n",
            "  Adjusted Rand Index: 0.5584\n",
            "  Mutual Information: 0.6994\n",
            "----------------------------------------\n",
            "\n",
            "Running Custom and Sklearn MS on AI Global Index Dataset\n",
            "----- AI Global Index -----\n",
            "Custom MS: Number of clusters = 7\n",
            "Sklearn MS: Number of clusters = 2\n",
            "\n",
            "Custom MS Metrics:\n",
            "  Silhouette: 0.1175\n",
            "  Davies-Bouldin: 1.5615\n",
            "  Calinski-Harabasz: 5.9750\n",
            "  Diameter: 1.9084\n",
            "  Split: 20.0000\n",
            "  Adjusted Rand Index: 0.0179\n",
            "  Mutual Information: 0.2412\n",
            "\n",
            "Sklearn MS Metrics:\n",
            "  Silhouette: 0.1629\n",
            "  Davies-Bouldin: 0.7229\n",
            "  Calinski-Harabasz: 1.7988\n",
            "  Diameter: 1.8901\n",
            "  Split: 61.0000\n",
            "  Adjusted Rand Index: 0.0513\n",
            "  Mutual Information: 0.1036\n",
            "----------------------------------------\n",
            "\n",
            "Running Custom and Sklearn MS on Earthquake Dataset\n",
            "----- Earthquake -----\n",
            "Custom MS: Number of clusters = 7\n",
            "Sklearn MS: Number of clusters = 9\n",
            "\n",
            "Custom MS Metrics:\n",
            "  Silhouette: -0.3594\n",
            "  Davies-Bouldin: 2.0205\n",
            "  Calinski-Harabasz: 15.3777\n",
            "  Diameter: 2.7858\n",
            "  Split: 751.0000\n",
            "  Adjusted Rand Index: -0.0264\n",
            "  Mutual Information: 0.0059\n",
            "\n",
            "Sklearn MS Metrics:\n",
            "  Silhouette: 0.4045\n",
            "  Davies-Bouldin: 0.7761\n",
            "  Calinski-Harabasz: 76.9291\n",
            "  Diameter: 3.9795\n",
            "  Split: 685.0000\n",
            "  Adjusted Rand Index: 0.1806\n",
            "  Mutual Information: 0.2031\n",
            "----------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def select_top_k_features_for_alert(df, alert_col='alert', k=5):\n",
        "    \"\"\"\n",
        "    Filters features based on Mutual Information (MI) with the alert label.\n",
        "    Steps:\n",
        "      1) Drop rows with any missing values (to avoid errors).\n",
        "      2) Label-encode the alert column if it is categorical (e.g., text).\n",
        "      3) For each feature, compute MI against the alert.\n",
        "      4) Sort features by MI score, descending.\n",
        "      5) Return the top k feature names.\n",
        "\n",
        "    \"\"\"\n",
        "    data = df.dropna()\n",
        "\n",
        "    # Separate the target (alert) from the features\n",
        "    y = data[alert_col]\n",
        "    X_df = data.drop(columns=[alert_col])\n",
        "\n",
        "    # encode alert:\n",
        "    y_enc = LabelEncoder().fit_transform(y)\n",
        "\n",
        "    # mutual_info_classif calculates MI between each column and y_enc\n",
        "    mi_scores = mutual_info_classif(X_df.values, y_enc, discrete_features=False)\n",
        "\n",
        "    # Pair up (feature_name, mi_score)\n",
        "    feats_and_scores = list(zip(X_df.columns, mi_scores))\n",
        "\n",
        "    # Sort by MI descending\n",
        "    feats_and_scores.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # Pick top k\n",
        "    top_features = [f[0] for f in feats_and_scores[:k]]\n",
        "\n",
        "    return top_features"
      ],
      "metadata": {
        "id": "1iEtvpuct0Bl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.2 - Select the top 5 features for predicting alert"
      ],
      "metadata": {
        "id": "54-R9VJ4y1F0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "top5_features = select_top_k_features_for_alert(\n",
        "    earthquake_df,  # the original DataFrame with 'alert' and other columns\n",
        "    alert_col='alert',\n",
        "    k=5\n",
        ")\n",
        "print(\"Top 5 features most predictive of 'alert':\", top5_features)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mP4mpzJEy5xK",
        "outputId": "36d10e40-8600-40e6-9f6b-e8282c82aaba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 features most predictive of 'alert': ['sig', 'mmi', 'longitude', 'latitude', 'magnitude']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.3 - Re-run the clustering using only top-5 features"
      ],
      "metadata": {
        "id": "PZ1F7Htey6K2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Subset the DataFrame to those top-5 columns\n",
        "from sklearn.cluster import estimate_bandwidth\n",
        "\n",
        "earthquake_df_top5 = earthquake_df[top5_features]\n",
        "bandwidth = estimate_bandwidth(earthquake_data, quantile=0.4)\n",
        "\n",
        "# 2) Scale the new subset\n",
        "eq_top5_data = StandardScaler().fit_transform(earthquake_df_top5.values)\n",
        "\n",
        "print(\"Running Base/Sklearn Affinity on Earthquake Dataset with Top-5 Features\")\n",
        "_ = run_experiment(\n",
        "    \"Earthquake (Top-5 Features)\",\n",
        "    eq_top5_data,\n",
        "    bandwidth=bandwidth,  # Smaller bandwidth for fine-grained clusters\n",
        "    true_labels=alert_encoded\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RO3tAxPhy_xD",
        "outputId": "7b07e772-a6ff-4685-9620-bc2f4c9d2627"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Base/Sklearn Affinity on Earthquake Dataset with Top-5 Features\n",
            "----- Earthquake (Top-5 Features) -----\n",
            "Custom MS: Number of clusters = 17\n",
            "Sklearn MS: Number of clusters = 2\n",
            "\n",
            "Custom MS Metrics:\n",
            "  Silhouette: -0.3665\n",
            "  Davies-Bouldin: 1.5267\n",
            "  Calinski-Harabasz: 3.2254\n",
            "  Diameter: 0.8234\n",
            "  Split: 664.0000\n",
            "  Adjusted Rand Index: -0.0865\n",
            "  Mutual Information: 0.0217\n",
            "\n",
            "Sklearn MS Metrics:\n",
            "  Silhouette: 0.6368\n",
            "  Davies-Bouldin: 0.5429\n",
            "  Calinski-Harabasz: 139.2829\n",
            "  Diameter: 6.2431\n",
            "  Split: 62.6667\n",
            "  Adjusted Rand Index: 0.3440\n",
            "  Mutual Information: 0.3464\n",
            "----------------------------------------\n"
          ]
        }
      ]
    }
  ]
}